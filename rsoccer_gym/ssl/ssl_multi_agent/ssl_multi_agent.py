import numpy as np
from gymnasium.spaces import Box, Dict
from rsoccer_gym.Entities import Ball, Frame, Robot
from rsoccer_gym.ssl.ssl_gym_base import SSLBaseEnv
from rsoccer_gym.Utils import KDTree
import random
from collections import namedtuple
from ray.rllib.env.multi_agent_env import MultiAgentEnv
from collections import OrderedDict
from rsoccer_gym.Entities.Robot import Robot
import copy
from gymnasium.wrappers import RecordVideo, FrameStackObservation
import random
from rsoccer_gym.Utils.Utils import Geometry2D
from rsoccer_gym.judges.ssl_judge import Judge
import math

class SSLMultiAgentEnv(SSLBaseEnv, MultiAgentEnv):
    default_players = 3
    def __init__(self,
        judge: Judge,
        init_pos,
        field_type=2, 
        fps=40,
        match_time=40,
        render_mode='human',
        dense_rewards = {},
        sparse_rewards = {},
        possession_radius_scale=3,
        direction_change_threshold=1,
    ):

        self.class_judge = judge
        self.possession_radius_scale = possession_radius_scale
        self.direction_change_threshold = direction_change_threshold
        self.n_robots_blue = min(len(init_pos["blue"]), 3)
        self.n_robots_yellow = min(len(init_pos["yellow"]), 3)
        self.score = {'blue': 0, 'yellow': 0}
        self.render_mode = render_mode
        super().__init__(
            field_type=field_type, 
            n_robots_blue=min(len(init_pos["blue"]), 3),
            n_robots_yellow=min(len(init_pos["yellow"]), 3), 
            time_step=1/fps,
            render_mode=render_mode
        )
        self.n_robots_blue = min(len(init_pos["blue"]), 3)
        self.n_robots_yellow = min(len(init_pos["yellow"]), 3)
        self.score = {'blue': 0, 'yellow': 0}
        self.field_info = {
            "length": self.field.length,
            "width": self.field.width,
            "goal_width": self.field.goal_width
        }
        self.dense_rewards = dense_rewards
        self.sparse_rewards = sparse_rewards
        self.render_mode = render_mode
        self.geometry = Geometry2D(
            -self.field.length/2, 
            self.field.length/2, 
            -self.field.width/2, 
            self.field.width/2
        )
        self.goal_template = namedtuple('goal', ['x', 'y'])
        self.ball_template = namedtuple('ball', ['x', 'y', 'v_x', 'v_y'])
        self._agent_ids = [
            *[f'blue_{i}'for i in range(self.n_robots_blue)], 
            *[f'yellow_{i}'for i in range(self.n_robots_yellow)]
        ]
        self.max_ep_length = int(match_time*fps)
        self.fps = fps
        # Limit robot speeds
        self.max_v = 1.5
        self.max_w = 10
        self.kick_speed_x = 3.0

        self.init_pos = init_pos

        self.obs_size = 3 #obs[f'blue_0'].shape[0]
        self.act_size = 4

        self.actions_bound = {"low": -1, "high": 1}

        blue = {f'blue_{i}': Box(
            low=self.actions_bound["low"], 
            high=self.actions_bound["high"], 
            shape=(self.act_size, ), 
            dtype=np.float64) for i in range(self.n_robots_blue)}
        yellow = {f'yellow_{i}': Box(
            low=self.actions_bound["low"], 
            high=self.actions_bound["high"], 
            shape=(self.act_size, ), 
            dtype=np.float64) for i in range(self.n_robots_yellow)}
        self.action_space =  Dict(**blue, **yellow)

        blue = {f'blue_{i}': Box(
                    low=max(-self.field.length/2, -self.field.width/2), 
                    high=min(self.field.length/2, self.field.width/2),
                    shape=(self.obs_size, ), 
                    dtype=np.float64) for i in range(self.n_robots_blue)}
        yellow = {f'yellow_{i}': Box(
                    low=max(-self.field.length/2, -self.field.width/2), 
                    high=min(self.field.length/2, self.field.width/2),
                    shape=(self.obs_size, ), 
                    dtype=np.float64) for i in range(self.n_robots_yellow)}
        self.observation_space = Dict(**blue, **yellow)



        self.judge_last_status, self.judge_last_info = dict(), dict()
        self.judge_status, self.judge_info = dict(), dict()
        init_frame = self._get_initial_positions_frame("kickoff")
        self.judge = self.class_judge(
            field=self.field, 
            initial_frame=init_frame, 
            possession_radius_scale=possession_radius_scale, 
            direction_change_threshold=direction_change_threshold
        )

    def _get_commands(self, actions):
        commands = []
        for i in range(self.n_robots_blue):
            robot_actions = actions[f'blue_{i}'].copy()
            angle = self.frame.robots_blue[i].theta
            v_x, v_y, v_theta = self.convert_actions(robot_actions, np.deg2rad(angle))
            cmd = Robot(yellow=False, id=i, v_x=v_x, v_y=v_y, v_theta=v_theta, kick_v_x=self.kick_speed_x * max(robot_actions[3], 0))
            commands.append(cmd)
        
        for i in range(self.n_robots_yellow):
            robot_actions = actions[f'yellow_{i}'].copy()
            angle = self.frame.robots_yellow[i].theta
            v_x, v_y, v_theta = self.convert_actions(robot_actions, np.deg2rad(angle))

            cmd = Robot(yellow=True, id=i, v_x=v_x, v_y=v_y, v_theta=v_theta, kick_v_x=self.kick_speed_x * max(robot_actions[3], 0))
            commands.append(cmd)

        return commands
    
    def convert_actions(self, action, angle):
        """Denormalize, clip to absolute max and convert to local"""

        # Denormalize
        v_x = action[0] * self.max_v
        v_y = action[1] * self.max_v
        v_theta = action[2] * self.max_w
        # Convert to local
        v_x, v_y = v_x*np.cos(angle) + v_y*np.sin(angle),\
            -v_x*np.sin(angle) + v_y*np.cos(angle)

        # clip by max absolute
        v_norm = np.linalg.norm([v_x,v_y])
        c = v_norm < self.max_v or self.max_v / v_norm
        v_x, v_y = v_x*c, v_y*c
        
        return v_x, v_y, v_theta


    def _calculate_reward_done(self):
        self.judge_last_status = self.judge_status
        self.judge_last_info = self.judge_info
        self.judge_status, self.judge_info = self.judge.judge(self.frame)

        done = {'__all__': False}
        truncated = {'__all__': False}

        reward_agents = {
            **{f"blue_{idx}":  0 for idx in range(self.n_robots_blue)},
            **{f"yellow_{idx}": 0 for idx in range(self.n_robots_yellow)},
        }
        for weight, reward_func, list_attr in self.dense_rewards:
            kwargs = {attr: getattr(self, attr) for attr in list_attr}
            reward_result = reward_func(
                self.field_info, self.observation, self.last_observation, 
                left="blue", right="yellow", 
                **kwargs
            )

            for agent, reward in reward_result.items():
                reward_agents[agent] += weight * reward

        ball = self.frame.ball
        last_touch = self.judge_info["last_touch"]
        map_freekick = {
            "RIGHT_BOTTOM_LINE_blue": [self.field.length/2 - 1, (self.field.width/2 - 0.2) * (1 if ball.y > 0 else -1)],
            "RIGHT_BOTTOM_LINE_yellow": [self.field.length/2 - 0.2, (self.field.width/2 - 0.2) * (1 if ball.y > 0 else -1)],
            "LEFT_BOTTOM_LINE_blue": [-self.field.length/2 + 0.2, (self.field.width/2 - 0.2) * (1 if ball.y > 0 else -1)],
            "LEFT_BOTTOM_LINE_yellow": [-self.field.length/2 + 1, (self.field.width/2 - 0.2) * (1 if ball.y > 0 else -1)]
        }

        if self.judge_status == "RIGHT_GOAL":
            done = {'__all__': True}
            self.score['blue'] += 1

            reward_agents.update({f'blue_{i}': self.sparse_rewards.get("GOAL_REWARD", 0) for i in range(self.n_robots_blue)})
            reward_agents.update({f'yellow_{i}': -self.sparse_rewards.get("GOAL_REWARD", 0)for i in range(self.n_robots_yellow)})
        
        elif self.judge_status == "LEFT_GOAL":
            done = {'__all__': True}
            self.score['yellow'] += 1

            reward_agents.update({f'blue_{i}': -self.sparse_rewards.get("GOAL_REWARD", 0) for i in range(self.n_robots_blue)})
            reward_agents.update({f'yellow_{i}': self.sparse_rewards.get("GOAL_REWARD", 0) for i in range(self.n_robots_yellow)})


        elif self.judge_status in ["LOWER_SIDELINE", "UPPER_SIDELINE"]:
            #reward_agents.update({last_touch: self.sparse_rewards.get("OUTSIDE_REWARD", 0) for i in range(self.n_robots_blue)})
            reward_agents.update({f"blue_{i}": self.sparse_rewards.get("OUTSIDE_REWARD", 0) for i in range(self.n_robots_blue)})
            reward_agents.update({f"yellow_{i}": self.sparse_rewards.get("OUTSIDE_REWARD", 0) for i in range(self.n_robots_yellow)})
                
            limit = self.field.length/2 - 0.2
            dx = max(abs(ball.x) - limit, 0) * (-1 if ball.x > 0 else 1)
            dy = -0.2  if ball.y > 0 else 0.2
            initial_pos_frame: Frame = self._get_initial_positions_frame(
                "freekick", 
                ball_pos=[ball.x + dx, ball.y + dy], 
                team_freekick= "yellow" if "blue" in last_touch else "blue"
            )
            self.rsim.reset(initial_pos_frame)
            self.frame = self.rsim.get_frame()
        
        elif self.judge_status in ["RIGHT_BOTTOM_LINE", "LEFT_BOTTOM_LINE"]:
            #reward_agents.update({last_touch: self.sparse_rewards.get("OUTSIDE_REWARD", 0) for i in range(self.n_robots_blue)})
            reward_agents.update({f"blue_{i}": self.sparse_rewards.get("OUTSIDE_REWARD", 0) for i in range(self.n_robots_blue)})
            reward_agents.update({f"yellow_{i}": self.sparse_rewards.get("OUTSIDE_REWARD", 0) for i in range(self.n_robots_yellow)})
        
            initial_pos_frame: Frame = self._get_initial_positions_frame(
                "freekick", 
                ball_pos=map_freekick[self.judge_status + "_" + last_touch.split("_")[0]],
                team_freekick="yellow" if "blue" in last_touch else "blue"
            )
            self.rsim.reset(initial_pos_frame)
            self.frame = self.rsim.get_frame()

        
        double_touch = False
        for robot_name, offenses in self.judge_info["offenses"].items():
            if len(offenses) == 0: continue
            reward_agents[robot_name] = 0
            for offense in offenses:
                if offense == "DOUBLE_TOUCH":
                    double_touch = True
                elif offense in ["OPPONENT_DEFENSE_AREA", "TEAM_DEFENSE_AREA"]:
                    done = {'__all__': True} # Analise if it should be done or not
                reward_agents[robot_name] += self.sparse_rewards.get(offense, 0)

        # if double_touch:
        #     initial_pos_frame: Frame = self._get_initial_positions_frame(
        #         "freekick", 
        #         ball_pos=[ball.x, ball.y],
        #         team_freekick="yellow" if "blue" in last_touch else "blue",
        #         use_init_pos=True
        #     )
        #     self.rsim.reset(initial_pos_frame)
        #     self.frame = self.rsim.get_frame()
        return reward_agents, done, truncated

    def reset(self, seed=42, options={}):
        self.steps = 0
        self.last_frame = None
        self.sent_commands = None
        self.last_observation = None


        self.judge_last_status, self.judge_last_info = dict(), dict()
        self.judge_status, self.judge_info = dict(), dict()
        init_frame = self._get_initial_positions_frame("kickoff")
        self.judge = self.class_judge(
            field=self.field, 
            initial_frame=init_frame, 
            possession_radius_scale=self.possession_radius_scale, 
            direction_change_threshold=self.direction_change_threshold
        )
        self.rsim.reset(init_frame)

        # Get frame from simulator
        self.frame = self.rsim.get_frame()

        blue = {f'blue_{i}': {} for i in range(self.n_robots_blue)}
        yellow = {f'yellow_{i}':{} for i in range(self.n_robots_yellow)}
        info = {**blue, **yellow}
        self.observation = self._frame_to_observations()
        self.score = {'blue': 0, 'yellow': 0}

        return self.observation.copy(), info
  
    def _get_initial_positions_frame(self, stage, ball_pos=None, team_freekick=None, use_init_pos=False):
        '''Returns the position of each robot and ball for the initial frame'''
        #np.random.seed(seed)

        field_half_length = self.field.length / 2
        field_half_width = self.field.width / 2

        def random(lim1, lim2): return np.random.uniform(lim1, lim2)

        places = KDTree()

        pos_frame: Frame = Frame()

        if stage == "kickoff":
            if isinstance(self.init_pos["ball"], list): 
                pos_frame.ball = Ball(x=self.init_pos["ball"][0], y=self.init_pos["ball"][1])
            else:
                pos_frame.ball = Ball(x=random.uniform(-2, 2), y=random.uniform(-1.2, 1.2))
            places.insert((pos_frame.ball.x, pos_frame.ball.y))

            min_dist = 0.2
            for i in range(self.n_robots_blue + self.n_robots_yellow):
                is_blue = i < self.n_robots_blue
                idx = i if is_blue else i - self.n_robots_blue
                color = "blue" if is_blue else "yellow"
                pos = self.init_pos[color][idx+1] 
                while places.get_nearest(pos[:2])[1] < min_dist:
                    pos = (
                        random(-field_half_length + 0.1, field_half_length - 0.1), 
                        random(-field_half_width + 0.1, field_half_width - 0.1), 
                        random(0, 360)
                    ) 
                places.insert(pos)

                pos_frame.robots_blue[i] = Robot(x=pos[0], y=pos[1], theta=pos[2])
                robot_list = getattr(pos_frame, f"robots_{color}")
                robot_list[idx] = Robot(x=pos[0], y=pos[1], theta=pos[2])

            
            last_touch = self.judge_info.get("last_touch", "")
            last_touch = "" if last_touch is None else last_touch
            team_last_touch =  last_touch.split("_")[0]
            kickoff_team = np.random.choice(["blue", "yellow"])
            if team_last_touch == "yellow":
                kickoff_team = "blue"
            elif team_last_touch == "blue":
                kickoff_team = "yellow"

            robots_list = getattr(pos_frame, f"robots_{kickoff_team}")
            robots_list[0] = Robot(
                x= 0.2 * -(robots_list[0].x / abs(robots_list[0].x)), 
                y= 0.0, 
                theta= robots_list[0].theta + 180.0
            )


        elif stage == "freekick":
            if ball_pos is None: raise ValueError("ball_pos must be provided for freekick")
            if team_freekick not in ["blue", "yellow"]: raise ValueError("team_freekick must be 'blue' or 'yellow'")
            pos_frame.ball = Ball(x=ball_pos[0], y=ball_pos[1])
            places.insert((pos_frame.ball.x, pos_frame.ball.y))

            min_dist_robots = 0.2
            min_dist_ball = 0.5
            for i in range(self.n_robots_blue + self.n_robots_yellow):
                is_blue = i < self.n_robots_blue
                idx = i if is_blue else i - self.n_robots_blue
                color = "blue" if is_blue else "yellow"
                x_lim = [-field_half_length + 0.1, 0] if is_blue else [0, field_half_length - 0.1]
                pos = (
                    random(x_lim[0], x_lim[1]) if not use_init_pos else self.init_pos[color][idx+1][0],
                    random(-field_half_width + 0.1, field_half_width - 0.1) if not use_init_pos else self.init_pos[color][idx+1][1],
                    random(0, 360) if not use_init_pos else self.init_pos[color][idx+1][2]
                ) 
                while (math.hypot(pos[0] - pos_frame.ball.x, pos[1] - pos_frame.ball.y) < min_dist_ball 
                       or places.get_nearest(pos[:2])[1] < min_dist_robots):
                    pos = (
                        random(x_lim[0], x_lim[1]),
                        random(-field_half_width + 0.1, field_half_width - 0.1),
                        random(0, 360)
                    ) 
                places.insert(pos)

                color = "blue" if is_blue else "yellow"
                robot_list = getattr(pos_frame, f"robots_{color}")
                robot_list[idx] = Robot(x=pos[0], y=pos[1], theta=pos[2])

            robots_list = getattr(pos_frame, f"robots_{team_freekick}")
            r = 0.2
            f = lambda x:  math.sqrt(r**2 - x**2)
            dx = random(0, r) if team_freekick == "yellow" else random(-r, 0)
            dy = f(dx) if ball_pos[1] > 0 else -f(dx)
            robots_list[0] = Robot(
                x= ball_pos[0] + dx, 
                y= ball_pos[1] + dy, 
                theta= random(0, 360)
            )

        return pos_frame
    
    def _frame_to_observations(self):
        rblue = self.frame.robots_blue
        ryellow = self.frame.robots_yellow
        observation = {
            **{f"blue_{i}": {"x": rblue[i].x, "y": rblue[i].y, "theta": rblue[i].theta} for i in range(self.n_robots_blue)},
            **{f"yellow_{i}": {"x": ryellow[i].x, "y": ryellow[i].y, "theta": ryellow[i].theta} for i in range(self.n_robots_yellow)},
            "ball": {"x": self.frame.ball.x, "y": self.frame.ball.y}
        }
        return observation
    
    def step(self, action):
        self.steps += 1
        # Join agent action with environment actions
        commands = self._get_commands(action)
        # Send command to simulator
        self.rsim.send_commands(commands)
        self.sent_commands = commands

        # Get Frame from simulator
        self.last_frame = self.frame
        self.frame = self.rsim.get_frame()

        # Calculate environment observation, reward and done condition
        self.last_observation = self.observation
        self.observation = self._frame_to_observations()
        
        reward, done, truncated = self._calculate_reward_done()

        if self.steps >= self.max_ep_length:
            done = {'__all__': False}
            truncated = {'__all__': True}

        infos = {
            **{f'blue_{i}': {} for i in range(self.n_robots_blue)},
            **{f'yellow_{i}': {} for i in range(self.n_robots_yellow)}
        }  

        if done.get("__all__", False) or truncated.get("__all__", False):
            for i in range(self.n_robots_blue):
                infos[f'blue_{i}']["score"] = self.score.copy()    

            for i in range(self.n_robots_yellow):
                infos[f'yellow_{i}']["score"] = self.score.copy()
        
        return self.observation.copy(), reward, done, truncated, infos

class SSLMultiAgentEnv_record(RecordVideo, MultiAgentEnv):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._agent_ids = self.env._agent_ids